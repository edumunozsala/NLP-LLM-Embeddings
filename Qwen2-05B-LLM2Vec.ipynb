{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu0YJ-pshqg2"
   },
   "source": [
    "# Transforming LLMs into High-Quality Text Embeddings with LLM2Vec.\n",
    "\n",
    "In the recent paper, \"LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders\" (August 2024), they introduce LLM2Vec, a straightforward, unsupervised method that transforms any decoder-only LLM into a powerful text encoder. \n",
    "\n",
    "This technique allows for embedding generation from decoder-based models like GPT, leveraging three key steps:\n",
    "✅ Bidirectional Attention: By replacing the typical causal attention with an all-ones matrix, each token can now attend to every other token in the sequence, giving it a “bidirectional” view.\n",
    "\n",
    "☑️ Masked Next Token Prediction (MNTP): MNTP combines next-token prediction with masked language modeling to build context awareness. It predicts masked tokens in the sequence while calculating loss based on the logits from previous positions, strengthening the model’s contextual understanding.\n",
    "\n",
    "❎ Unsupervised Contrastive Learning: Using SimCSE, this step helps the model create distinct representations by maximizing similarity between different representations of the same sentence while minimizing similarity with representations of others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is inspired in the article: [Llama 3.2 Embeddings: Training and Evaluation with LLM2Vec](https://newsletter.kaitchup.com/p/llama-32-embeddings-training) and its notebook.* by Benjamin Marie.\n",
    "\n",
    "I also reviewed the *LLM2Vec Github* repo where you can find examples using other language models.\n",
    "\n",
    "\n",
    "In this notebook, we will see how to make text embeddings from Qwen2 0.5 B. We will see in detail all the steps: masked next-token prediction training, contrastive learning, and then how to evaluate the resulting embeddings.\n",
    "You can find the base model on Huggingface, [Qwen2 0.5B Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)\n",
    "\n",
    "To train and evaluate the embedding model, I used an RTX 3090 from Vast.ai.\n",
    "\n",
    "\n",
    "## Sections\n",
    "\n",
    "* Package installation\n",
    "* Masked next-token prediction training\n",
    "* Contrastive learning\n",
    "* Merging to the base model and saving the adapter to Huggingface Hub\n",
    "* Evaluation the model\n",
    "* Downloading the model and make some inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuLb25FIiB4E"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47664,
     "status": "ok",
     "timestamp": 1732379338796,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "d3HuKm13AwSt",
    "outputId": "675ce455-3f34-468a-c1e8-aa2e74d88728",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llm2vec'...\n",
      "remote: Enumerating objects: 915, done.\u001b[K\n",
      "remote: Counting objects: 100% (345/345), done.\u001b[K\n",
      "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
      "remote: Total 915 (delta 244), reused 197 (delta 197), pack-reused 570 (from 1)\u001b[K\n",
      "Receiving objects: 100% (915/915), 1.48 MiB | 6.35 MiB/s, done.\n",
      "Resolving deltas: 100% (532/532), done.\n",
      "Obtaining file:///workspace/llm2vec\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<=4.44.2,>=4.43.1\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting mteb>=1.14.12\n",
      "  Downloading mteb-1.21.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from mteb>=1.14.12) (2.31.0)\n",
      "Collecting scipy>=0.0.0 (from mteb>=1.14.12)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentence_transformers>=3.0.0 (from mteb>=1.14.12)\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from mteb>=1.14.12) (4.9.0)\n",
      "Collecting rich>=0.0.0 (from mteb>=1.14.12)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pytrec-eval-terrier>=0.5.6 (from mteb>=1.14.12)\n",
      "  Downloading pytrec_eval_terrier-0.5.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (777 bytes)\n",
      "Collecting pydantic>=2.0.0 (from mteb>=1.14.12)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.8/170.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting eval_type_backport>=0.0.0 (from mteb>=1.14.12)\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting polars>=0.20.22 (from mteb>=1.14.12)\n",
      "  Downloading polars-1.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.26.0 (from mteb>=1.14.12)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.44.2,>=4.43.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers<=4.44.2,>=4.43.1)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<=4.44.2,>=4.43.1)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic>=2.0.0->mteb>=1.14.12)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic>=2.0.0->mteb>=1.14.12)\n",
      "  Downloading pydantic_core-2.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from mteb>=1.14.12)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb>=1.14.12) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb>=1.14.12) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb>=1.14.12) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb>=1.14.12) (2023.11.17)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=0.0.0->mteb>=1.14.12)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=0.0.0->mteb>=1.14.12) (2.15.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers>=3.0.0->mteb>=1.14.12) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=0.0.0->mteb>=1.14.12)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading mteb-1.21.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading aiohttp-3.11.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.6/447.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading polars-1.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytrec_eval_terrier-0.5.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m656.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m643.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m722.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m564.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m318.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m782.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m853.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m711.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.4/319.4 kB\u001b[0m \u001b[31m626.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xxhash, tzdata, typing-extensions, tqdm, threadpoolctl, scipy, safetensors, requests, regex, pytrec-eval-terrier, pyarrow, propcache, polars, mdurl, joblib, frozenlist, eval_type_backport, dill, async-timeout, annotated-types, aiohappyeyeballs, scikit-learn, pydantic-core, pandas, multiprocess, multidict, markdown-it-py, huggingface-hub, aiosignal, yarl, tokenizers, rich, pydantic, accelerate, transformers, aiohttp, sentence_transformers, peft, datasets, mteb, evaluate, llm2vec\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Running setup.py develop for llm2vec\n",
      "Successfully installed accelerate-1.1.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.8 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-5.0.1 datasets-2.21.0 dill-0.3.8 eval_type_backport-0.2.0 evaluate-0.4.3 frozenlist-1.5.0 huggingface-hub-0.26.3 joblib-1.4.2 llm2vec-0.2.2 markdown-it-py-3.0.0 mdurl-0.1.2 mteb-1.21.1 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.2 polars-1.16.0 propcache-0.2.0 pyarrow-18.1.0 pydantic-2.10.2 pydantic-core-2.27.1 pytrec-eval-terrier-0.5.6 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence_transformers-3.3.1 threadpoolctl-3.5.0 tokenizers-0.19.1 tqdm-4.67.1 transformers-4.44.2 typing-extensions-4.12.2 tzdata-2024.2 xxhash-3.5.0 yarl-1.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting flash-attn\n",
      "  Downloading flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m971.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.2.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m818.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m--:--\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=183192450 sha256=4f950453f76c9d45f3ee5c066bcbf927b06669418517784a8b900a2e8c50140f\n",
      "  Stored in directory: /root/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.7.0.post2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/McGill-NLP/llm2vec.git\n",
    "!cd llm2vec && pip install -e .[evaluation]\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn4gLKBEiOMz"
   },
   "source": [
    "You will need an access token from Hugging Face to be able to Qwen 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5114,
     "status": "ok",
     "timestamp": 1732379432783,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "Gb-Fw6jM2_Ys"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "hf_token= userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token='<YOUR HUGGINGFACE API KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1282,
     "status": "ok",
     "timestamp": 1732379450835,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "PwGbqYhTDIAs"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=hf_token) #enter you Hugging Face access token here to be able to use Llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0mkiG_fiUt3"
   },
   "source": [
    "# Masked next-token prediction training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model and training parameters and sabve them to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1732379722554,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "ccsqMPlm0WVM"
   },
   "outputs": [],
   "source": [
    "JSON_CONFIG='''\n",
    " {\n",
    "    \"model_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"dataset_name\": \"wikitext\",\n",
    "    \"dataset_config_name\": \"wikitext-103-raw-v1\",\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"do_train\": true,\n",
    "    \"do_eval\": true,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"mask_token_type\": \"blank\",\n",
    "    \"data_collator_type\": \"default\",\n",
    "    \"mlm_probability\": 0.2,\n",
    "    \"overwrite_output_dir\": true,\n",
    "    \"output_dir\": \"output/mntp/Qwen2-0.5B\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 250,\n",
    "    \"stop_after_n_steps\": 1000,\n",
    "    \"lora_r\": 16,\n",
    "    \"gradient_checkpointing\": true,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"attn_implementation\": \"flash_attention_2\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"dataloader_prefetch_factor\": 2\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(\"mtnp_qwen2_config.json\", 'w') as f:\n",
    "  f.write(JSON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the MNTP training using the code provided by LLM2Vec. LLM2Vec does this training with LoRA. We only train an adapter that we will load for the next steps. It makes training relatively cheap. As you can see our training dataset is the \"WikiText 103\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1005356,
     "status": "ok",
     "timestamp": 1732380729536,
     "user": {
      "displayName": "Eduardo Muñoz Sala",
      "userId": "13317831924226771761"
     },
     "user_tz": -60
    },
    "id": "ib_kkYzRDN93",
    "outputId": "119b5718-e721-4772-9acb-5fee2bc768ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "12/01/2024 10:05:49 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/01/2024 10:05:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=2,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=100,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/mntp/Qwen2-0.5B/runs/Dec01_10-05-49_abd31d806443,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=output/mntp/Qwen2-0.5B,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/mntp/Qwen2-0.5B,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=250,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f.incomplete\n",
      "12/01/2024 10:05:50 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f.incomplete\n",
      "Downloading readme: 100%|██████████████████| 10.5k/10.5k [00:00<00:00, 39.1kB/s]\n",
      "storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md in cache at /root/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f\n",
      "12/01/2024 10:05:51 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/README.md in cache at /root/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f\n",
      "creating metadata file for /root/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f\n",
      "12/01/2024 10:05:51 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d782d8e09126a8b9159d20854e89213c4d49aa22a54ec37e62d0613de365611f\n",
      "Generating dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n",
      "12/01/2024 10:05:54 - INFO - datasets.builder - Generating dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n",
      "Downloading and preparing dataset wikitext/wikitext-103-raw-v1 to /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3...\n",
      "12/01/2024 10:05:54 - INFO - datasets.builder - Downloading and preparing dataset wikitext/wikitext-103-raw-v1 to /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3...\n",
      "hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/82acc81c7f527ffe803bcd4e380d51fe2ea54585abaa026255183b318777c838.incomplete\n",
      "12/01/2024 10:05:54 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/82acc81c7f527ffe803bcd4e380d51fe2ea54585abaa026255183b318777c838.incomplete\n",
      "Downloading data: 100%|██████████████████████| 733k/733k [00:00<00:00, 1.00MB/s]\n",
      "storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/82acc81c7f527ffe803bcd4e380d51fe2ea54585abaa026255183b318777c838\n",
      "12/01/2024 10:05:55 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/82acc81c7f527ffe803bcd4e380d51fe2ea54585abaa026255183b318777c838\n",
      "creating metadata file for /root/.cache/huggingface/datasets/downloads/82acc81c7f527ffe803bcd4e380d51fe2ea54585abaa026255183b318777c838\n",
      "12/01/2024 10:05:55 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/82acc81c7f527ffe803bcd4e380d51fe2ea54585abaa026255183b318777c838\n",
      "hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00000-of-00002.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/1ace5b3e360b99429e0997814d8bb83095d0209e95f1f281d090ff7cbe653443.incomplete\n",
      "12/01/2024 10:05:56 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00000-of-00002.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/1ace5b3e360b99429e0997814d8bb83095d0209e95f1f281d090ff7cbe653443.incomplete\n",
      "Downloading data: 100%|██████████████████████| 157M/157M [00:08<00:00, 19.3MB/s]\n",
      "storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00000-of-00002.parquet in cache at /root/.cache/huggingface/datasets/downloads/1ace5b3e360b99429e0997814d8bb83095d0209e95f1f281d090ff7cbe653443\n",
      "12/01/2024 10:06:04 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00000-of-00002.parquet in cache at /root/.cache/huggingface/datasets/downloads/1ace5b3e360b99429e0997814d8bb83095d0209e95f1f281d090ff7cbe653443\n",
      "creating metadata file for /root/.cache/huggingface/datasets/downloads/1ace5b3e360b99429e0997814d8bb83095d0209e95f1f281d090ff7cbe653443\n",
      "12/01/2024 10:06:04 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/1ace5b3e360b99429e0997814d8bb83095d0209e95f1f281d090ff7cbe653443\n",
      "hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00001-of-00002.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/002a846870c7ffa34b907de946a2932a93ffbf09554db783053a88e549092775.incomplete\n",
      "12/01/2024 10:06:04 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00001-of-00002.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/002a846870c7ffa34b907de946a2932a93ffbf09554db783053a88e549092775.incomplete\n",
      "Downloading data: 100%|██████████████████████| 157M/157M [00:08<00:00, 18.7MB/s]\n",
      "storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00001-of-00002.parquet in cache at /root/.cache/huggingface/datasets/downloads/002a846870c7ffa34b907de946a2932a93ffbf09554db783053a88e549092775\n",
      "12/01/2024 10:06:13 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/train-00001-of-00002.parquet in cache at /root/.cache/huggingface/datasets/downloads/002a846870c7ffa34b907de946a2932a93ffbf09554db783053a88e549092775\n",
      "creating metadata file for /root/.cache/huggingface/datasets/downloads/002a846870c7ffa34b907de946a2932a93ffbf09554db783053a88e549092775\n",
      "12/01/2024 10:06:13 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/002a846870c7ffa34b907de946a2932a93ffbf09554db783053a88e549092775\n",
      "hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/7adcb5f80fc45c6d031e50594a5f5a03202c29ea08ddecbb402ab1813827beef.incomplete\n",
      "12/01/2024 10:06:13 - INFO - datasets.utils.file_utils - hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/7adcb5f80fc45c6d031e50594a5f5a03202c29ea08ddecbb402ab1813827beef.incomplete\n",
      "Downloading data: 100%|██████████████████████| 657k/657k [00:00<00:00, 1.49MB/s]\n",
      "storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/7adcb5f80fc45c6d031e50594a5f5a03202c29ea08ddecbb402ab1813827beef\n",
      "12/01/2024 10:06:14 - INFO - datasets.utils.file_utils - storing hf://datasets/wikitext@b08601e04326c79dfdd32d625aee71d232d685c3/wikitext-103-raw-v1/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/7adcb5f80fc45c6d031e50594a5f5a03202c29ea08ddecbb402ab1813827beef\n",
      "creating metadata file for /root/.cache/huggingface/datasets/downloads/7adcb5f80fc45c6d031e50594a5f5a03202c29ea08ddecbb402ab1813827beef\n",
      "12/01/2024 10:06:14 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/7adcb5f80fc45c6d031e50594a5f5a03202c29ea08ddecbb402ab1813827beef\n",
      "Downloading took 0.0 min\n",
      "12/01/2024 10:06:14 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "12/01/2024 10:06:14 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Generating test split\n",
      "12/01/2024 10:06:14 - INFO - datasets.builder - Generating test split\n",
      "Generating test split: 100%|█████| 4358/4358 [00:00<00:00, 242591.40 examples/s]\n",
      "Generating train split\n",
      "12/01/2024 10:06:14 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 100%|█| 1801350/1801350 [00:02<00:00, 735481.86 examples\n",
      "Generating validation split\n",
      "12/01/2024 10:06:16 - INFO - datasets.builder - Generating validation split\n",
      "Generating validation split: 100%|█| 3760/3760 [00:00<00:00, 776990.84 examples/\n",
      "All the splits matched successfully.\n",
      "12/01/2024 10:06:16 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3. Subsequent calls will reuse this data.\n",
      "12/01/2024 10:06:16 - INFO - datasets.builder - Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3. Subsequent calls will reuse this data.\n",
      "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 8.43MB/s]\n",
      "[INFO|configuration_utils.py:733] 2024-12-01 10:06:17,285 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-12-01 10:06:17,286 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 18.4MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 8.03MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 22.3MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 18.9MB/s]\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-12-01 10:06:19,589 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-12-01 10:06:19,589 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-12-01 10:06:19,590 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-12-01 10:06:19,590 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-12-01 10:06:19,590 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-12-01 10:06:19,590 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-12-01 10:06:19,799 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors: 100%|█████████████████████| 988M/988M [00:37<00:00, 26.1MB/s]\n",
      "[INFO|modeling_utils.py:3678] 2024-12-01 10:06:58,226 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-12-01 10:06:58,253 >> Instantiating Qwen2BiForMNTP model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:328] 2024-12-01 10:06:58,255 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:1038] 2024-12-01 10:06:58,257 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4507] 2024-12-01 10:06:58,659 >> All model checkpoint weights were used when initializing Qwen2BiForMNTP.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-12-01 10:06:58,659 >> All the weights of Qwen2BiForMNTP were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2BiForMNTP for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 3.11MB/s]\n",
      "[INFO|configuration_utils.py:993] 2024-12-01 10:06:58,959 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-12-01 10:06:58,959 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "Model's Lora trainable parameters:\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
      "Running tokenizer on every text in dataset:   0%| | 0/4358 [00:00<?, ? examples/Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8cb554d0909ba489.arrow\n",
      "12/01/2024 10:06:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-8cb554d0909ba489.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 4358/4358 [00:00<00:00, 8185\n",
      "Running tokenizer on every text in dataset:   0%| | 0/1801350 [00:00<?, ? examplCaching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-add8b1ed2ab3d2b3.arrow\n",
      "12/01/2024 10:07:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-add8b1ed2ab3d2b3.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 1801350/1801350 [02:47<00:00\n",
      "Running tokenizer on every text in dataset:   0%| | 0/3760 [00:00<?, ? examples/Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-06cd87789e1017b4.arrow\n",
      "12/01/2024 10:09:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-06cd87789e1017b4.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 3760/3760 [00:00<00:00, 1108\n",
      "Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7f1515b70f646f51.arrow\n",
      "12/01/2024 10:09:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7f1515b70f646f51.arrow\n",
      "Grouping texts in chunks of 512: 100%|█| 4358/4358 [00:00<00:00, 7012.19 example\n",
      "Grouping texts in chunks of 512:   0%|       | 0/1801350 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b93df83cfa3f75e0.arrow\n",
      "12/01/2024 10:09:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-b93df83cfa3f75e0.arrow\n",
      "Grouping texts in chunks of 512: 100%|█| 1801350/1801350 [04:16<00:00, 7020.64 e\n",
      "Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5f8352f87dee57f0.arrow\n",
      "12/01/2024 10:14:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-5f8352f87dee57f0.arrow\n",
      "Grouping texts in chunks of 512: 100%|█| 3760/3760 [00:00<00:00, 6970.40 example\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 7.05MB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:575: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:2134] 2024-12-01 10:14:07,771 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-12-01 10:14:07,771 >>   Num examples = 241,770\n",
      "[INFO|trainer.py:2136] 2024-12-01 10:14:07,771 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2137] 2024-12-01 10:14:07,771 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2140] 2024-12-01 10:14:07,771 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2141] 2024-12-01 10:14:07,771 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:2142] 2024-12-01 10:14:07,771 >>   Total optimization steps = 22,665\n",
      "[INFO|trainer.py:2143] 2024-12-01 10:14:07,776 >>   Number of trainable parameters = 8,798,208\n",
      "  0%|                                                 | 0/22665 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[WARNING|logging.py:328] 2024-12-01 10:14:08,037 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  0%|▏                                   | 100/22665 [03:43<13:59:20,  2.23s/it][INFO|trainer.py:3819] 2024-12-01 10:17:50,996 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:17:50,996 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:17:50,996 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[WARNING|logging.py:328] 2024-12-01 10:17:51,190 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.98it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.18it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:06,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.98it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.97it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.8038980960845947, 'eval_accuracy': 0.4692538576980798, 'eval_runtime': 11.3038, 'eval_samples_per_second': 45.118, 'eval_steps_per_second': 2.831, 'epoch': 0.01}\n",
      "  0%|▏                                   | 100/22665 [03:54<13:59:20,  2.23s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.01it/s]\u001b[A\n",
      "  1%|▎                                   | 200/22665 [07:37<13:53:58,  2.23s/it][INFO|trainer.py:3819] 2024-12-01 10:21:45,012 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:21:45,013 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:21:45,013 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.96it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.17it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.36it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.21it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:08,  3.12it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.07it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.03it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.98it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.97it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.97it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.97it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.435026168823242, 'eval_accuracy': 0.5304961832061069, 'eval_runtime': 11.3281, 'eval_samples_per_second': 45.021, 'eval_steps_per_second': 2.825, 'epoch': 0.03}\n",
      "  1%|▎                                   | 200/22665 [07:48<13:53:58,  2.23s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.00it/s]\u001b[A\n",
      "  1%|▍                                   | 250/22665 [09:39<13:47:33,  2.22s/it]12/01/2024 10:23:47 - INFO - __main__ - Saving model checkpoint to output/mntp/Qwen2-0.5B/checkpoint-250\n",
      "[INFO|configuration_utils.py:733] 2024-12-01 10:23:48,037 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-12-01 10:23:48,041 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-12-01 10:23:48,120 >> tokenizer config file saved in output/mntp/Qwen2-0.5B/checkpoint-250/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-12-01 10:23:48,120 >> Special tokens file saved in output/mntp/Qwen2-0.5B/checkpoint-250/special_tokens_map.json\n",
      "  1%|▍                                   | 300/22665 [11:31<13:47:14,  2.22s/it][INFO|trainer.py:3819] 2024-12-01 10:25:39,634 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:25:39,634 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:25:39,634 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.98it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.19it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.02it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:06,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.99it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.97it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.97it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.2790274620056152, 'eval_accuracy': 0.5562959427207638, 'eval_runtime': 11.3318, 'eval_samples_per_second': 45.006, 'eval_steps_per_second': 2.824, 'epoch': 0.04}\n",
      "  1%|▍                                   | 300/22665 [11:43<13:47:14,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.98it/s]\u001b[A\n",
      "  2%|▋                                   | 400/22665 [15:25<13:42:40,  2.22s/it]\u001b[A[INFO|trainer.py:3819] 2024-12-01 10:29:33,399 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:29:33,399 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:29:33,399 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.97it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.18it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.99it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.96it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.96it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.1950974464416504, 'eval_accuracy': 0.5685026614209674, 'eval_runtime': 11.2958, 'eval_samples_per_second': 45.15, 'eval_steps_per_second': 2.833, 'epoch': 0.05}\n",
      "  2%|▋                                   | 400/22665 [15:36<13:42:40,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.01it/s]\u001b[A\n",
      "{'loss': 2.6444, 'grad_norm': 2.0921578407287598, 'learning_rate': 4.8896977718949924e-05, 'epoch': 0.07}\n",
      "  2%|▊                                   | 500/22665 [19:19<13:39:10,  2.22s/it][INFO|trainer.py:3819] 2024-12-01 10:33:27,255 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:33:27,255 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:33:27,255 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.95it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.17it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.98it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.97it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.97it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.96it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.136404514312744, 'eval_accuracy': 0.5778529423069532, 'eval_runtime': 11.3262, 'eval_samples_per_second': 45.028, 'eval_steps_per_second': 2.825, 'epoch': 0.07}\n",
      "  2%|▊                                   | 500/22665 [19:30<13:39:10,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.00it/s]\u001b[A\n",
      "                                                                                \u001b[A12/01/2024 10:33:38 - INFO - __main__ - Saving model checkpoint to output/mntp/Qwen2-0.5B/checkpoint-500\n",
      "[INFO|configuration_utils.py:733] 2024-12-01 10:33:39,083 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-12-01 10:33:39,086 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-12-01 10:33:39,194 >> tokenizer config file saved in output/mntp/Qwen2-0.5B/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-12-01 10:33:39,194 >> Special tokens file saved in output/mntp/Qwen2-0.5B/checkpoint-500/special_tokens_map.json\n",
      "  3%|▉                                   | 600/22665 [23:14<13:42:45,  2.24s/it][INFO|trainer.py:3819] 2024-12-01 10:37:22,131 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:37:22,132 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:37:22,132 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.98it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.18it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:08,  3.12it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.03it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.98it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.97it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.95it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.96it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.106135129928589, 'eval_accuracy': 0.5824118965946764, 'eval_runtime': 11.3339, 'eval_samples_per_second': 44.998, 'eval_steps_per_second': 2.823, 'epoch': 0.08}\n",
      "  3%|▉                                   | 600/22665 [23:25<13:42:45,  2.24s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.98it/s]\u001b[A\n",
      "  3%|█                                   | 700/22665 [27:08<13:36:30,  2.23s/it][INFO|trainer.py:3819] 2024-12-01 10:41:16,008 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:41:16,008 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:41:16,008 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.97it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.19it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.36it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:06,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.99it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.97it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.0628139972686768, 'eval_accuracy': 0.5903284496792109, 'eval_runtime': 11.3237, 'eval_samples_per_second': 45.038, 'eval_steps_per_second': 2.826, 'epoch': 0.09}\n",
      "  3%|█                                   | 700/22665 [27:19<13:36:30,  2.23s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.01it/s]\u001b[A\n",
      "  3%|█▏                                  | 750/22665 [29:10<13:30:28,  2.22s/it]12/01/2024 10:43:18 - INFO - __main__ - Saving model checkpoint to output/mntp/Qwen2-0.5B/checkpoint-750\n",
      "[INFO|configuration_utils.py:733] 2024-12-01 10:43:19,082 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-12-01 10:43:19,084 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-12-01 10:43:19,163 >> tokenizer config file saved in output/mntp/Qwen2-0.5B/checkpoint-750/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-12-01 10:43:19,163 >> Special tokens file saved in output/mntp/Qwen2-0.5B/checkpoint-750/special_tokens_map.json\n",
      "  4%|█▎                                  | 800/22665 [31:03<13:33:05,  2.23s/it][INFO|trainer.py:3819] 2024-12-01 10:45:10,818 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:45:10,818 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:45:10,818 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.96it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.18it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.21it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:08,  3.12it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.07it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.99it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.96it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.0277180671691895, 'eval_accuracy': 0.5955153165940958, 'eval_runtime': 11.3096, 'eval_samples_per_second': 45.094, 'eval_steps_per_second': 2.829, 'epoch': 0.11}\n",
      "  4%|█▎                                  | 800/22665 [31:14<13:33:05,  2.23s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.00it/s]\u001b[A\n",
      "  4%|█▍                                  | 900/22665 [34:56<13:27:32,  2.23s/it][INFO|trainer.py:3819] 2024-12-01 10:49:04,699 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:49:04,699 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:49:04,699 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.97it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.19it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.63it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.02it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:06,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.99it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.97it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.9819859266281128, 'eval_accuracy': 0.6020545833012104, 'eval_runtime': 11.3142, 'eval_samples_per_second': 45.076, 'eval_steps_per_second': 2.828, 'epoch': 0.12}\n",
      "  4%|█▍                                  | 900/22665 [35:08<13:27:32,  2.23s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.01it/s]\u001b[A\n",
      "{'loss': 2.0634, 'grad_norm': 2.1615307331085205, 'learning_rate': 4.7793955437899846e-05, 'epoch': 0.13}\n",
      "  4%|█▌                                 | 1000/22665 [38:50<13:23:18,  2.22s/it][INFO|trainer.py:3819] 2024-12-01 10:52:58,580 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:52:58,580 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:52:58,580 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▊                                         | 2/32 [00:00<00:05,  5.96it/s]\u001b[A\n",
      "  9%|████▏                                       | 3/32 [00:00<00:06,  4.18it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:01<00:07,  3.62it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:01<00:08,  3.37it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:01<00:08,  3.22it/s]\u001b[A\n",
      " 22%|█████████▋                                  | 7/32 [00:02<00:07,  3.13it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:02<00:07,  3.08it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:02<00:07,  3.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:03<00:07,  3.01it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:03<00:07,  3.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:03<00:06,  2.98it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:04<00:06,  2.97it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:04<00:06,  2.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:04<00:05,  2.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:05<00:05,  2.96it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:05<00:05,  2.97it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:05<00:04,  2.96it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:06<00:04,  2.96it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:06<00:03,  2.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:07<00:03,  2.96it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:07<00:02,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:08<00:02,  2.96it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:08<00:02,  2.97it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:08<00:01,  2.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:09<00:01,  2.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:09<00:01,  2.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:09<00:00,  2.97it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:10<00:00,  2.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.9695574045181274, 'eval_accuracy': 0.6058281617391975, 'eval_runtime': 11.3177, 'eval_samples_per_second': 45.062, 'eval_steps_per_second': 2.827, 'epoch': 0.13}\n",
      "  4%|█▌                                 | 1000/22665 [39:02<13:23:18,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  3.00it/s]\u001b[A\n",
      "                                                                                \u001b[A12/01/2024 10:53:09 - INFO - __main__ - Saving model checkpoint to output/mntp/Qwen2-0.5B/checkpoint-1000\n",
      "[INFO|configuration_utils.py:733] 2024-12-01 10:53:10,588 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-12-01 10:53:10,592 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-12-01 10:53:10,699 >> tokenizer config file saved in output/mntp/Qwen2-0.5B/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-12-01 10:53:10,700 >> Special tokens file saved in output/mntp/Qwen2-0.5B/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:2394] 2024-12-01 10:53:11,067 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2343.2917, 'train_samples_per_second': 309.526, 'train_steps_per_second': 9.672, 'train_loss': 2.353895751953125, 'epoch': 0.13}\n",
      "  4%|█▌                                 | 1000/22665 [39:03<14:06:07,  2.34s/it]\n",
      "12/01/2024 10:53:11 - INFO - __main__ - Saving model checkpoint to output/mntp/Qwen2-0.5B\n",
      "[INFO|configuration_utils.py:733] 2024-12-01 10:53:11,479 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-12-01 10:53:11,481 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-12-01 10:53:11,578 >> tokenizer config file saved in output/mntp/Qwen2-0.5B/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-12-01 10:53:11,578 >> Special tokens file saved in output/mntp/Qwen2-0.5B/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.1324\n",
      "  total_flos               = 33572050GF\n",
      "  train_loss               =     2.3539\n",
      "  train_runtime            = 0:39:03.29\n",
      "  train_samples            =     241770\n",
      "  train_samples_per_second =    309.526\n",
      "  train_steps_per_second   =      9.672\n",
      "12/01/2024 10:53:11 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3819] 2024-12-01 10:53:11,805 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-12-01 10:53:11,805 >>   Num examples = 510\n",
      "[INFO|trainer.py:3824] 2024-12-01 10:53:11,805 >>   Batch size = 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.96it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     0.1324\n",
      "  eval_accuracy           =     0.6022\n",
      "  eval_loss               =     1.9832\n",
      "  eval_runtime            = 0:00:11.37\n",
      "  eval_samples            =        510\n",
      "  eval_samples_per_second =      44.85\n",
      "  eval_steps_per_second   =      2.814\n",
      "  perplexity              =      7.266\n"
     ]
    }
   ],
   "source": [
    "!python llm2vec/experiments/run_mntp.py mtnp_qwen2_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0hpQUOyiaWa"
   },
   "source": [
    "# Contrastive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE-Rb1ZEidWq"
   },
   "source": [
    "We need to download the training dataset, Wikitext 103 for the Simce training. For this learning, LLM2Vec uses SimCSE (Simple Contrastive Learning of Sentence Embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HC1ExSBhDkky",
    "outputId": "c0230b3d-c243-49e5-d025-3b71690ad65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-01 10:53:48--  https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\n",
      "Resolving huggingface.co (huggingface.co)... 13.32.110.109, 13.32.110.55, 13.32.110.77, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.32.110.109|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf.co/datasets/princeton-nlp/datasets-for-simcse/7b1825863a99aa76479b0456f7c210539dfaeeb69598b41fb4de4f524dd5a706?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27wiki1m_for_simcse.txt%3B+filename%3D%22wiki1m_for_simcse.txt%22%3B&response-content-type=text%2Fplain&Expires=1733309628&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMzMwOTYyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9wcmluY2V0b24tbmxwL2RhdGFzZXRzLWZvci1zaW1jc2UvN2IxODI1ODYzYTk5YWE3NjQ3OWIwNDU2ZjdjMjEwNTM5ZGZhZWViNjk1OThiNDFmYjRkZTRmNTI0ZGQ1YTcwNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=J3mCpdmO1v4vu24T9YPfC1BPJWY4Ds1FnAkof6zBpRkC8ic9kD1eVutgX98wpGTKMjrm5B7hzVotLDdpkr4e9%7Ebp3lwBzEdbODccyNqd3Ce72qNl0NG6BEd2-oUMb50CGceXOZUEvE%7E7tGb4M2folfvTg0PfavYohsJISDnz-FPFsabNXqvz5HoQMIL2gfOS0kKokbjIOsi5l0DyQ9fVAIrIS%7ELSq9USrcKi4rg99HFR9YuHMcNGtVI2NbOODrhx%7E3M8Jwk2NElvzWeUe6xldL05nXEBqFuVQm3D8BoOY-Ub7Y94jalobWmuW1P3%7EKVPgu9DjoMfDclvd4Q03YMeiA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2024-12-01 10:53:48--  https://cdn-lfs.hf.co/datasets/princeton-nlp/datasets-for-simcse/7b1825863a99aa76479b0456f7c210539dfaeeb69598b41fb4de4f524dd5a706?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27wiki1m_for_simcse.txt%3B+filename%3D%22wiki1m_for_simcse.txt%22%3B&response-content-type=text%2Fplain&Expires=1733309628&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMzMwOTYyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9wcmluY2V0b24tbmxwL2RhdGFzZXRzLWZvci1zaW1jc2UvN2IxODI1ODYzYTk5YWE3NjQ3OWIwNDU2ZjdjMjEwNTM5ZGZhZWViNjk1OThiNDFmYjRkZTRmNTI0ZGQ1YTcwNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=J3mCpdmO1v4vu24T9YPfC1BPJWY4Ds1FnAkof6zBpRkC8ic9kD1eVutgX98wpGTKMjrm5B7hzVotLDdpkr4e9%7Ebp3lwBzEdbODccyNqd3Ce72qNl0NG6BEd2-oUMb50CGceXOZUEvE%7E7tGb4M2folfvTg0PfavYohsJISDnz-FPFsabNXqvz5HoQMIL2gfOS0kKokbjIOsi5l0DyQ9fVAIrIS%7ELSq9USrcKi4rg99HFR9YuHMcNGtVI2NbOODrhx%7E3M8Jwk2NElvzWeUe6xldL05nXEBqFuVQm3D8BoOY-Ub7Y94jalobWmuW1P3%7EKVPgu9DjoMfDclvd4Q03YMeiA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 108.138.7.100, 108.138.7.33, 108.138.7.105, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|108.138.7.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 120038621 (114M) [text/plain]\n",
      "Saving to: ‘wiki1m_for_simcse.txt’\n",
      "\n",
      "wiki1m_for_simcse.t 100%[===================>] 114.48M  19.6MB/s    in 6.0s    \n",
      "\n",
      "2024-12-01 10:53:54 (19.1 MB/s) - ‘wiki1m_for_simcse.txt’ saved [120038621/120038621]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new json file containing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nfSl6G2h5R6k"
   },
   "outputs": [],
   "source": [
    "JSON_CONFIG='''\n",
    "{\n",
    "    \"model_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"peft_model_name_or_path\": \"output/mntp/Qwen2-0.5B\",\n",
    "    \"simcse_dropout\": 0.3,\n",
    "    \"bidirectional\": true,\n",
    "    \"pooling_mode\": \"mean\",\n",
    "    \"dataset_name\": \"Wiki1M\",\n",
    "    \"dataset_file_path\": \"wiki1m_for_simcse.txt\",\n",
    "    \"remove_unused_columns\": false,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"loss_scale\": 20,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"do_train\": true,\n",
    "    \"disable_tqdm\": false,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"overwrite_output_dir\": true,\n",
    "    \"output_dir\": \"output/mntp-simcse/Qwen2-0.5B\",\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_steps\": 250,\n",
    "    \"save_only_model\": true,\n",
    "    \"stop_after_n_steps\": 1000,\n",
    "    \"lora_r\": 16,\n",
    "    \"gradient_checkpointing\": true,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"attn_implementation\": \"flash_attention_2\",\n",
    "    \"seed\": 422\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(\"simcse_qwen2_config.json\", 'w') as f:\n",
    "  f.write(JSON_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to run the contrastive learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AcOanVbBEBVV",
    "outputId": "d3471caf-336c-4b36-8d62-3905bba69f91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 10:57:55 - llm2vec.dataset.Wiki1M - INFO - Loading Wiki1M data from wiki1m_for_simcse.txt...\n",
      "2024-12-01 10:57:57 - llm2vec.dataset.Wiki1M - INFO - Loaded 1000000 samples.\n",
      "Loading train examples...: 100%|██| 1000000/1000000 [00:03<00:00, 320123.83it/s]\n",
      "2024-12-01 10:58:03 - peft.tuners.tuners_utils - INFO - Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "Model's Lora trainable parameters:\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
      "{'loss': 0.8613, 'grad_norm': 103.16072082519531, 'learning_rate': 2.9935995903737838e-05, 'epoch': 0.01}\n",
      "{'loss': 0.1583, 'grad_norm': 32.38646697998047, 'learning_rate': 2.987199180747568e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0821, 'grad_norm': 20.49149513244629, 'learning_rate': 2.9807987711213516e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0574, 'grad_norm': 21.583349227905273, 'learning_rate': 2.9743983614951356e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0484, 'grad_norm': 36.00017547607422, 'learning_rate': 2.9679979518689197e-05, 'epoch': 0.03}\n",
      "  1%|▍                                   | 250/23436 [12:47<18:58:51,  2.95s/it]2024-12-01 11:10:53 - __main__ - INFO - Saving model checkpoint to output/mntp-simcse/Qwen2-0.5B/checkpoint-250\n",
      "{'loss': 0.0488, 'grad_norm': 10.738896369934082, 'learning_rate': 2.9615975422427037e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0624, 'grad_norm': 9.209731101989746, 'learning_rate': 2.9551971326164875e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0396, 'grad_norm': 8.255395889282227, 'learning_rate': 2.9487967229902715e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0341, 'grad_norm': 7.327056884765625, 'learning_rate': 2.9423963133640552e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0319, 'grad_norm': 2.8435182571411133, 'learning_rate': 2.9359959037378393e-05, 'epoch': 0.06}\n",
      "  2%|▊                                   | 500/23436 [25:36<19:19:38,  3.03s/it]2024-12-01 11:23:42 - __main__ - INFO - Saving model checkpoint to output/mntp-simcse/Qwen2-0.5B/checkpoint-500\n",
      "{'loss': 0.0326, 'grad_norm': 10.232004165649414, 'learning_rate': 2.929595494111623e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0312, 'grad_norm': 10.080077171325684, 'learning_rate': 2.923195084485407e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0399, 'grad_norm': 9.296765327453613, 'learning_rate': 2.9167946748591908e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0251, 'grad_norm': 7.334811210632324, 'learning_rate': 2.9103942652329752e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0245, 'grad_norm': 19.97072410583496, 'learning_rate': 2.903993855606759e-05, 'epoch': 0.1}\n",
      "  3%|█▏                                  | 750/23436 [38:30<18:52:22,  2.99s/it]2024-12-01 11:36:35 - __main__ - INFO - Saving model checkpoint to output/mntp-simcse/Qwen2-0.5B/checkpoint-750\n",
      "{'loss': 0.0307, 'grad_norm': 8.11484432220459, 'learning_rate': 2.897593445980543e-05, 'epoch': 0.1}\n",
      "{'loss': 0.0221, 'grad_norm': 3.2494301795959473, 'learning_rate': 2.8911930363543267e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0154, 'grad_norm': 0.40513625741004944, 'learning_rate': 2.8847926267281107e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0038, 'grad_norm': 1.0917357206344604, 'learning_rate': 2.8783922171018945e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0017, 'grad_norm': 0.06017693132162094, 'learning_rate': 2.8719918074756785e-05, 'epoch': 0.13}\n",
      "  4%|█▍                                 | 1000/23436 [51:23<18:54:23,  3.03s/it]2024-12-01 11:49:28 - __main__ - INFO - Saving model checkpoint to output/mntp-simcse/Qwen2-0.5B/checkpoint-1000\n",
      "{'train_runtime': 3083.8361, 'train_samples_per_second': 972.814, 'train_steps_per_second': 7.6, 'train_loss': 0.08255860663950443, 'epoch': 0.13}\n",
      "  4%|█▍                                 | 1000/23436 [51:23<19:13:08,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "!python llm2vec/experiments/run_simcse.py simcse_qwen2_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHxemwUsikXu"
   },
   "source": [
    "# Merging the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/llm2vec\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (2.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (2.5.1+cu124)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (0.13.2)\n",
      "Requirement already satisfied: transformers<=4.44.2,>=4.43.1 in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (4.44.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (2.21.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from llm2vec==0.2.2) (1.5.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->llm2vec==0.2.2) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->llm2vec==0.2.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->llm2vec==0.2.2) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets->llm2vec==0.2.2) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets->llm2vec==0.2.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->llm2vec==0.2.2) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->llm2vec==0.2.2) (3.11.8)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft->llm2vec==0.2.2) (6.1.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from peft->llm2vec==0.2.2) (1.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->llm2vec==0.2.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->llm2vec==0.2.2) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->llm2vec==0.2.2) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->llm2vec==0.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->llm2vec==0.2.2) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->llm2vec==0.2.2) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.44.2,>=4.43.1->llm2vec==0.2.2) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->llm2vec==0.2.2) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->llm2vec==0.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->llm2vec==0.2.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->llm2vec==0.2.2) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->llm2vec==0.2.2) (1.16.0)\n",
      "Installing collected packages: llm2vec\n",
      "  Attempting uninstall: llm2vec\n",
      "    Found existing installation: llm2vec 0.2.2\n",
      "    Uninstalling llm2vec-0.2.2:\n",
      "      Successfully uninstalled llm2vec-0.2.2\n",
      "\u001b[33m  DEPRECATION: Legacy editable install of llm2vec==0.2.2 from file:///workspace/llm2vec (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py develop for llm2vec\n",
      "Successfully installed llm2vec-0.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd llm2vec && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the adapter to the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llm2vec.llm2vec import LLM2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8TpPgoVDJSnk"
   },
   "outputs": [],
   "source": [
    "l2v_model = LLM2Vec.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    peft_model_name_or_path=\"output/mntp-simcse/Qwen2-0.5B/checkpoint-1000/\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    merge_peft=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained adapter to disk and later to the HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2v_model.save(\"Qwen2-0.5B-mntp-simcse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9553e00c245bf967a526dea081474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 502 thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/68/e4/68e4108e4ec796f8f61eba28ea62eb9f2d21b1c90183d11e762bb99445398781/08802948e85b7176a2dbaf30a1b2b44debde0e624713515731162bb6f4c2569a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20241201%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241201T115147Z&X-Amz-Expires=86400&X-Amz-Signature=1f6543b28a59dc9dc7153f6e3022df4ca352d1b374f053220b1c9216e5de13a8&X-Amz-SignedHeaders=host&partNumber=14&uploadId=BOvj0uiTqUp4F5Axk1H_yk7YqW3j.96zQo21p3_aiDJwIe2XHIMUhwPhnAAsz7Y0IVN_2Xvz3pdLKu5411gYr.ZexYAs1bhXeQ5EmYw1JcNNA4qQONk5mk_LW.p7iI_b&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/edumunozsala/Qwen2-0.5B-mntp-simcse/commit/a6317a00246c6bcaf80d121226fb3d1fc0aa40fa', commit_message='Upload model', commit_description='', oid='a6317a00246c6bcaf80d121226fb3d1fc0aa40fa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/edumunozsala/Qwen2-0.5B-mntp-simcse', endpoint='https://huggingface.co', repo_type='model', repo_id='edumunozsala/Qwen2-0.5B-mntp-simcse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2v_model.model.push_to_hub(\"edumunozsala/Qwen2-0.5B-mntp-simcse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mta3fQgCJSnl"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ce460e3db841ba8e7c8472aae15853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "# Encoding queries using instructions\n",
    "instruction = (\n",
    "    \"Given a web search query, retrieve relevant passages that answer the query:\"\n",
    ")\n",
    "queries = [\n",
    "    [instruction, \"how much protein should a female eat\"],\n",
    "    [instruction, \"summit define\"],\n",
    "]\n",
    "q_reps = l2v_model.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666d2715fb7546e98995e0d5eff12b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encoding documents. Instruction are not required for documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "d_reps = l2v_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8209, 0.5430],\n",
      "        [0.6533, 0.7960]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.6470, 0.1619],\\n        [0.0786, 0.5844]])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine similarity\n",
    "q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1)\n",
    "d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1)\n",
    "cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1))\n",
    "\n",
    "print(cos_sim)\n",
    "\"\"\"\n",
    "tensor([[0.6470, 0.1619],\n",
    "        [0.0786, 0.5844]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQoqk_xoimIH"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf1gDVNviozy"
   },
   "source": [
    "The Massive Text Embedding Benchmark (MTEB) is an evaluation framework designed to assess the effectiveness of text embeddings across diverse tasks, datasets, and languages. It includes 8 tasks, 58 datasets, and 112 languages.\n",
    "\n",
    "It includes tasks like clustering, reranking, classification, semantic textual similarity (STS), and retrieval to ensure a holistic assessment of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sWDheVyaJSnk",
    "outputId": "369b6512-3846-4ba3-9472-6560a86581ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mteb==1.14.10\n",
      "  Downloading mteb-1.14.10-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: datasets>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (2.21.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (1.26.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (1.5.2)\n",
      "Requirement already satisfied: scipy>=0.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (1.14.1)\n",
      "Requirement already satisfied: sentence-transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (4.12.2)\n",
      "Requirement already satisfied: torch>1.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (2.2.0)\n",
      "Requirement already satisfied: tqdm>1.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (4.67.1)\n",
      "Requirement already satisfied: rich>=0.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (13.9.4)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.6 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (0.5.6)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (2.10.2)\n",
      "Requirement already satisfied: eval-type-backport>=0.0.0 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (0.2.0)\n",
      "Requirement already satisfied: polars>=0.20.22 in /opt/conda/lib/python3.10/site-packages (from mteb==1.14.10) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.19.0->mteb==1.14.10) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (0.26.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.19.0->mteb==1.14.10) (6.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->mteb==1.14.10) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->mteb==1.14.10) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb==1.14.10) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb==1.14.10) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb==1.14.10) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->mteb==1.14.10) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=0.0.0->mteb==1.14.10) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=0.0.0->mteb==1.14.10) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->mteb==1.14.10) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->mteb==1.14.10) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=3.0.0->mteb==1.14.10) (4.44.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=3.0.0->mteb==1.14.10) (10.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>1.0.0->mteb==1.14.10) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>1.0.0->mteb==1.14.10) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>1.0.0->mteb==1.14.10) (3.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.19.0->mteb==1.14.10) (1.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=0.0.0->mteb==1.14.10) (0.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=3.0.0->mteb==1.14.10) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=3.0.0->mteb==1.14.10) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=3.0.0->mteb==1.14.10) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>1.0.0->mteb==1.14.10) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.19.0->mteb==1.14.10) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.19.0->mteb==1.14.10) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.19.0->mteb==1.14.10) (2024.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>1.0.0->mteb==1.14.10) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->mteb==1.14.10) (1.16.0)\n",
      "Downloading mteb-1.14.10-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m379.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mteb\n",
      "  Attempting uninstall: mteb\n",
      "    Found existing installation: mteb 1.21.1\n",
      "    Uninstalling mteb-1.21.1:\n",
      "      Successfully uninstalled mteb-1.21.1\n",
      "Successfully installed mteb-1.14.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mteb==1.14.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sSfVnAVi3Gp"
   },
   "source": [
    "Evaluation on all the STS tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "etlWCTA1EFzZ",
    "outputId": "6b22593c-bc4b-4517-b4da-1205b68ea494",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS16, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                           | 0/38 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 38/38 [00:03<00:00, 10.75it/s]\n",
      "Batches: 100%|██████████████████████████████████| 38/38 [00:02<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS13, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                           | 0/47 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 47/47 [00:04<00:00, 11.29it/s]\n",
      "Batches: 100%|██████████████████████████████████| 47/47 [00:03<00:00, 13.83it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS14, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                          | 0/118 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|████████████████████████████████| 118/118 [00:09<00:00, 12.40it/s]\n",
      "Batches: 100%|████████████████████████████████| 118/118 [00:08<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS15, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                           | 0/94 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 94/94 [00:07<00:00, 12.13it/s]\n",
      "Batches: 100%|██████████████████████████████████| 94/94 [00:07<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS17, \u001b[3;90ms2s\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m11\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m11\u001b[0m\u001b[3;31m Subsets\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                           | 0/89 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 89/89 [00:07<00:00, 11.34it/s]\n",
      "Batches: 100%|██████████████████████████████████| 89/89 [00:07<00:00, 12.39it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.93it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.96it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.07it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.82it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.97it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.09it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.06it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.94it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.96it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.85it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.02it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.03it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.88it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.03it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.68it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.05it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.91it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.69it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 12.97it/s]\n",
      "Batches: 100%|████████████████████████████████████| 8/8 [00:00<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS22, \u001b[3;90mp2p\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m18\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m18\u001b[0m\u001b[3;31m Subsets\u001b[0m\n",
      "\n",
      "\n",
      "WARNING:mteb.abstasks.AbsTask:Dataset 'STS22' is superseeded by 'STS22.v2', you might consider using the newer version of the dataset.\n",
      "Batches:   0%|                                           | 0/17 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 17/17 [00:04<00:00,  3.73it/s]\n",
      "Batches: 100%|██████████████████████████████████| 17/17 [00:03<00:00,  4.72it/s]\n",
      "Batches: 100%|████████████████████████████████████| 6/6 [00:01<00:00,  4.03it/s]\n",
      "Batches: 100%|████████████████████████████████████| 6/6 [00:01<00:00,  4.14it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.50it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.39it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.14it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.45it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  3.69it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.14it/s]\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00,  3.99it/s]\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:00<00:00,  4.21it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  3.83it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████████████████████████████| 21/21 [00:04<00:00,  4.53it/s]\n",
      "Batches: 100%|██████████████████████████████████| 21/21 [00:04<00:00,  4.61it/s]\n",
      "Batches: 100%|████████████████████████████████████| 9/9 [00:02<00:00,  4.01it/s]\n",
      "Batches: 100%|████████████████████████████████████| 9/9 [00:02<00:00,  4.24it/s]\n",
      "Batches: 100%|████████████████████████████████████| 5/5 [00:01<00:00,  3.98it/s]\n",
      "Batches: 100%|████████████████████████████████████| 5/5 [00:01<00:00,  4.39it/s]\n",
      "Batches: 100%|████████████████████████████████████| 2/2 [00:00<00:00,  4.13it/s]\n",
      "Batches: 100%|████████████████████████████████████| 2/2 [00:00<00:00,  5.19it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.15it/s]\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:01<00:00,  4.50it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:00<00:00,  4.27it/s]\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:00<00:00,  4.49it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  9.68it/s]\n",
      "Batches: 100%|██████████████████████████████████| 13/13 [00:03<00:00,  4.12it/s]\n",
      "Batches: 100%|██████████████████████████████████| 13/13 [00:02<00:00,  4.37it/s]\n",
      "Batches: 100%|██████████████████████████████████| 12/12 [00:03<00:00,  3.55it/s]\n",
      "Batches: 100%|██████████████████████████████████| 12/12 [00:03<00:00,  3.91it/s]\n",
      "Batches: 100%|████████████████████████████████████| 6/6 [00:01<00:00,  4.11it/s]\n",
      "Batches: 100%|████████████████████████████████████| 6/6 [00:01<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STS12, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                           | 0/98 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 98/98 [00:08<00:00, 11.95it/s]\n",
      "Batches: 100%|██████████████████████████████████| 98/98 [00:07<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - BIOSSES, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                            | 0/4 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:01<00:00,  3.98it/s]\n",
      "Batches: 100%|████████████████████████████████████| 4/4 [00:00<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - STSBenchmark, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                           | 0/44 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|██████████████████████████████████| 44/44 [00:03<00:00, 11.08it/s]\n",
      "Batches: 100%|██████████████████████████████████| 44/44 [00:03<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models:Failed to extract metadata from model: 'LLM2VecWrapper' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n",
      "\u001b[30m─────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[30m ────────────────────────────────\u001b[0m\n",
      "\u001b[1mSTS\u001b[0m\n",
      "    - SICK-R, \u001b[3;90ms2s\u001b[0m\n",
      "\n",
      "\n",
      "Batches:   0%|                                          | 0/311 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Batches: 100%|████████████████████████████████| 311/311 [00:24<00:00, 12.91it/s]\n",
      "Batches: 100%|████████████████████████████████| 311/311 [00:23<00:00, 13.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in [\"STS16\",\"STS13\",\"STS14\",\"STS15\",\"STS17\",\"STS22\",\"STS12\",\"BIOSSES\",\"STSBenchmark\",\"SICK-R\"]:\n",
    "    !python llm2vec/experiments/mteb_eval_custom.py --base_model_name_or_path Qwen/Qwen2-0.5B-Instruct --peft_model_name_or_path output/mntp-simcse/Qwen2-0.5B/checkpoint-1000/ \\\n",
    "    --task_name {t} \\\n",
    "    --task_to_instructions_fp llm2vec/test_configs/mteb/task_to_instructions.json \\\n",
    "    --output_dir results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Adapter model to the base model from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading base Mistral model, along with custom code that enables bidirectional connections in decoder-only LLMs.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\", padding_side='left' \n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNTP (Masked Next Token Prediction) model.\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"output/mntp-simcse/Qwen2-0.5B/checkpoint-1000/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for encoding and pooling operations\n",
    "l2v = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8501a5536b354738a0bdeff0c99f6963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4748a63bfde049d3824771810cde359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8217, 0.5439],\n",
      "        [0.6554, 0.7964]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.6266, 0.4199],\\n        [0.3429, 0.5240]])\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding queries using instructions\n",
    "instruction = (\n",
    "    \"Given a web search query, retrieve relevant passages that answer the query:\"\n",
    ")\n",
    "queries = [\n",
    "    [instruction, \"how much protein should a female eat\"],\n",
    "    [instruction, \"summit define\"],\n",
    "]\n",
    "q_reps = l2v.encode(queries)\n",
    "\n",
    "# Encoding documents. Instruction are not required for documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "d_reps = l2v.encode(documents)\n",
    "\n",
    "# Compute cosine similarity\n",
    "q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1)\n",
    "d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1)\n",
    "cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1))\n",
    "\n",
    "print(cos_sim)\n",
    "\"\"\"\n",
    "tensor([[0.6266, 0.4199],\n",
    "        [0.3429, 0.5240]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to he Hugginface hub\n",
    "\n",
    "Now, we save the and the tokenizer to the Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce2f54d02804a33977b6cf09c241b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a080d82b8d14671bdc88da393c04412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/edumunozsala/Qwen2-0.5B-mntp-simcse/commit/6b2e3012e8a72923a15ff33306b7bcc8c3719f12', commit_message='Upload model', commit_description='', oid='6b2e3012e8a72923a15ff33306b7bcc8c3719f12', pr_url=None, repo_url=RepoUrl('https://huggingface.co/edumunozsala/Qwen2-0.5B-mntp-simcse', endpoint='https://huggingface.co', repo_type='model', repo_id='edumunozsala/Qwen2-0.5B-mntp-simcse'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"edumunozsala/Qwen2-0.5B-mntp-simcse\")\n",
    "tokenizer.push_to_hub(\"edumunozsala/Qwen2-0.5B-mntp-simcse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model saved in HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading base Mistral model, along with custom code that enables bidirectional connections in decoder-only LLMs.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\", padding_side='left' \n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNTP (Masked Next Token Prediction) model.\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"edumunozsala/Qwen2-0.5B-mntp-simcse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for encoding and pooling operations\n",
    "l2v = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46596193d61f4025990d08e94b1a572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab2328faeeb46f0a456cf9b94ab5349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8217, 0.5439],\n",
      "        [0.6554, 0.7964]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.6266, 0.4199],\\n        [0.3429, 0.5240]])\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding queries using instructions\n",
    "instruction = (\n",
    "    \"Given a web search query, retrieve relevant passages that answer the query:\"\n",
    ")\n",
    "queries = [\n",
    "    [instruction, \"how much protein should a female eat\"],\n",
    "    [instruction, \"summit define\"],\n",
    "]\n",
    "q_reps = l2v.encode(queries)\n",
    "\n",
    "# Encoding documents. Instruction are not required for documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "d_reps = l2v.encode(documents)\n",
    "\n",
    "# Compute cosine similarity\n",
    "q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1)\n",
    "d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1)\n",
    "cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1))\n",
    "\n",
    "print(cos_sim)\n",
    "\"\"\"\n",
    "tensor([[0.6266, 0.4199],\n",
    "        [0.3429, 0.5240]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1cxs_RJhUqS5qCDEeh02efSuBEYa0vsv1",
     "timestamp": 1731255259070
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
